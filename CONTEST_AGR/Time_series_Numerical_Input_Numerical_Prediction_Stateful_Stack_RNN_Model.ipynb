{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Time-series Numerical Input / Numerical Prediction - Stateful Stack RNN Model",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m8int_MpSA0",
        "outputId": "18ecc50f-47bf-4ded-f4d8-706daf80e9a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "1m8int_MpSA0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ordered-wichita"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "ordered-wichita",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mineral-twins"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "id": "mineral-twins",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "classical-launch"
      },
      "source": [
        "# gpu 설정\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "id": "classical-launch",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smart-resource"
      },
      "source": [
        "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/public_data/train.csv')"
      ],
      "id": "smart-resource",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desperate-multiple"
      },
      "source": [
        "# 평가식\n",
        "def nmae(y_true, y_pred):\n",
        "    score = np.mean(np.abs(y_true - y_pred) / y_true)\n",
        "    return score"
      ],
      "id": "desperate-multiple",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acknowledged-cambodia"
      },
      "source": [
        "# 정규화\n",
        "def normalize(data, col):\n",
        "    \n",
        "    # 요일은 원핫 인코딩을 적용함, 요일에 대한 의미가 없을 것이라고 판단\n",
        "    data = pd.concat([data, pd.get_dummies(data['요일'])], axis = 1)\n",
        "    data = data.drop(['요일'], axis = 1)\n",
        "\n",
        "    col1 = data.columns[-7:].to_list()\n",
        "    col2 = data.columns[1:-7].to_list()\n",
        "\n",
        "    new_col = ['date'] + col1 + col2\n",
        "    data = data[new_col]\n",
        "    data = data.drop(['date'], axis = 1)\n",
        "    # 0 ~ 1 값으로 정규화 진행\n",
        "    norm = data.iloc[:,8:].max(0)\n",
        "    data.iloc[:,8:] = data.iloc[:,8:]/norm\n",
        "    \n",
        "    train = data.iloc[:, col:col + 2]\n",
        "    \n",
        "    return train, norm"
      ],
      "id": "acknowledged-cambodia",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anticipated-feeding"
      },
      "source": [
        "# train & test 분리\n",
        "def load_data(data, window_size = 28, future_size = 28, train_size = 0.9):\n",
        "        x = []; y = []\n",
        "        for i in range(len(data) - window_size - future_size):\n",
        "            x.append(data.iloc[i: i+window_size])\n",
        "            y.append(data.iloc[i+window_size:i+window_size+future_size, 1]) # 가격만\n",
        "\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "\n",
        "        train_idx = round(len(x) * train_size)\n",
        "\n",
        "        train_x = x[:train_idx]\n",
        "        train_y = y[:train_idx]\n",
        "\n",
        "        valid_x = x[train_idx:]\n",
        "        valid_y = y[train_idx:]\n",
        "        \n",
        "        return train_x, train_y, valid_x, valid_y"
      ],
      "id": "anticipated-feeding",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suburban-metabolism"
      },
      "source": [
        "# 모델 구축\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    for i in range(2):\n",
        "        model.add(LSTM(28,stateful=True, return_sequences = True, batch_input_shape = (1,28, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "    model.add(LSTM(28,stateful=True, return_sequences = False))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(28))\n",
        "        \n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "id": "suburban-metabolism",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "better-gender"
      },
      "source": [
        "window_size = 28\n",
        "submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/sample_submission.csv')\n",
        "public_date_list = submission[submission['예측대상일자'].str.contains('2020')]['예측대상일자'].str.split('+').str[0].unique()"
      ],
      "id": "better-gender",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "eligible-superintendent",
        "outputId": "7a21449c-8033-4467-ca03-1ea5556846c3"
      },
      "source": [
        "for n, col in enumerate(list(range(7, 49, 2))):\n",
        "\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/public_data/train.csv')\n",
        "    train, norm = normalize(data, col)\n",
        "    \n",
        "    train_x, train_y, valid_x, valid_y = load_data(train)\n",
        "    \n",
        "    \n",
        "    model = build_model()\n",
        "    model.fit(train_x, train_y, batch_size=1, epochs=200, validation_data=(valid_x, valid_y), verbose=1, callbacks = [early_stop])\n",
        "    \n",
        "    y_pred = model.predict(valid_x)\n",
        "    y_true = valid_y\n",
        "\n",
        "    \n",
        "    target_idx = np.where(y_true != 0)\n",
        "    y_pred = y_pred[target_idx]\n",
        "    y_true = y_true[target_idx]\n",
        "    \n",
        "    print(train.columns[1], '의 NMAE: ', nmae(y_true, y_pred))\n",
        "    \n",
        "    ## 실제 test데이터 생성\n",
        "    for date in public_date_list:\n",
        "        test_df = pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/public_data/test_files/test_{date}.csv')\n",
        "        data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/public_data/train.csv')\n",
        "        data = pd.concat([data, test_df]).iloc[-window_size:]\n",
        "\n",
        "        test, norm = normalize(data, col)\n",
        "        sub_output = model.predict(test.to_numpy().reshape(1,28,2)) * norm[n*2]\n",
        "\n",
        "        idx = submission[submission['예측대상일자'].str.contains(date)].index\n",
        "        submission.loc[idx, train.columns[1]] = sub_output[0,[6,13,27]]\n",
        "    print(submission.iloc[:20,n+1])"
      ],
      "id": "eligible-superintendent",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1509/1509 [==============================] - 41s 25ms/step - loss: 0.0066 - val_loss: 0.0102\n",
            "Epoch 2/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0056 - val_loss: 0.0099\n",
            "Epoch 3/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0055 - val_loss: 0.0092\n",
            "Epoch 4/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0053 - val_loss: 0.0096\n",
            "Epoch 5/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0051 - val_loss: 0.0081\n",
            "Epoch 6/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0050 - val_loss: 0.0089\n",
            "Epoch 7/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0049 - val_loss: 0.0078\n",
            "Epoch 8/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0048 - val_loss: 0.0081\n",
            "Epoch 9/200\n",
            "1509/1509 [==============================] - 35s 23ms/step - loss: 0.0047 - val_loss: 0.0078\n",
            "Epoch 00009: early stopping\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d78e3572ef1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 957\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:    Specified a list with shape [1,2] from a tensor with shape [32,2]\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\n\t [[sequential_9/lstm_20/PartitionedCall]] [Op:__inference_predict_function_129707]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recreational-oakland"
      },
      "source": [
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/Data/Dacon_data/result/Time-series Numerical Input / Numerical Prediction - Stateful Stack RNN Model.csv', index = False)\n"
      ],
      "id": "recreational-oakland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyric-guatemala"
      },
      "source": [
        "submission"
      ],
      "id": "lyric-guatemala",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laden-acrylic"
      },
      "source": [
        "\n",
        "\n"
      ],
      "id": "laden-acrylic",
      "execution_count": null,
      "outputs": []
    }
  ]
}